import gradio as gr
import os
import glob
import hashlib


global SELECT_IMAGE_PATH
SELECT_IMAGE_PATH = ""  # Initialize global variable to store selected image path


def load_image():
    path = "examples/photos"
    image_paths = glob.glob(os.path.join(path, "*.png")) + glob.glob(
        os.path.join(path, "*.jpg")
    )
    return image_paths


def select_image(evt: gr.SelectData):
    """Handle image selection"""
    print(evt.value)
    global SELECT_IMAGE_PATH
    SELECT_IMAGE_PATH = evt.value["image"]["path"]  # Get the path of the selected image
    return evt.value["image"]["path"]  # Return selected image path


# Launch Gradio interface
with gr.Blocks(title="æ™ºèƒ½ç›¸å†Œ", theme=gr.themes.Soft()) as app:
    gr.Markdown("# ğŸ“· æ™ºèƒ½ç›¸å†Œåº”ç”¨")
    gr.Markdown("ç›¸å†Œå±•ç¤ºï¼Œé€‰æ‹©ï¼Œä¸Šä¼ ï¼Œç½‘ç»œå¤„ç†demo")

    with gr.Row():
        with gr.Column(scale=5):
            gallery = gr.Gallery(
                label="ç…§ç‰‡å¢™", columns=10, object_fit="cover", value=load_image()
            )

            # ratio = gr.Radio(
            #     label="å›¾ç‰‡æ¯”ä¾‹",
            #     choices=["src", "dest"],
            #     value="src",
            #     type="value"
            # )

        with gr.Column(scale=2):
            selected_image = gr.Image(label="é€‰ä¸­çš„å›¾ç‰‡1", interactive=False)
            process_btn = gr.Button("å¤„ç†é€‰ä¸­å›¾ç‰‡")
            # selected_image2 = gr.Image(label="å¤„ç†ç»“æœç¤ºæ„", interactive=False)

            # Track last selected image index
            selected_images = []

            def handle_selection(evt: gr.SelectData):
                path = select_image(evt)
                selected_images.append(path)
                return path, path

        with gr.Column(scale=2):
            from utils.caption import image_captioning

            upload = gr.Image(
                label="ä¸Šä¼ å›¾ç‰‡", type="pil"
            )  # ä½¿ç”¨ gr.Image ä¸Šä¼ å›¾åƒï¼Œç±»å‹ä¸º PIL å›¾åƒ
            upload_txt = gr.Textbox(label="ä¸Šä¼ ç»“æœ")  # æç¤ºæ–‡æœ¬æ¡†
            caption_txt = gr.Textbox(label="å›¾åƒæ€»ç»“")  # æç¤ºæ–‡æœ¬æ¡†

            from PIL import Image, ImageDraw
            import numpy as np

            def handle_imageprocess(image_array):
                """ä½¿ç”¨ PIL ç»™ numpy æ ¼å¼çš„å›¾åƒæ•°ç»„åŠ ä¸Šé®ç½©"""
                # å°† numpy æ•°ç»„è½¬æ¢ä¸º PIL å›¾åƒ
                image = Image.fromarray(image_array).convert("RGB")

                # åˆ›å»ºä¸€ä¸ªä¸å›¾åƒå¤§å°ç›¸åŒçš„é®ç½©
                mask = Image.new("L", image.size, 0)  # 'L' æ¨¡å¼è¡¨ç¤ºå•é€šé“ç°åº¦å›¾åƒ
                draw = ImageDraw.Draw(mask)

                # åœ¨é®ç½©ä¸Šç»˜åˆ¶ä¸€ä¸ªåœ†å½¢
                width, height = image.size
                radius = min(width, height) // 3  # åœ†çš„åŠå¾„
                center = (width // 2, height // 2)  # åœ†çš„ä¸­å¿ƒ
                draw.ellipse(
                    (
                        center[0] - radius,
                        center[1] - radius,
                        center[0] + radius,
                        center[1] + radius,
                    ),
                    fill=255,
                )

                # å°†é®ç½©åº”ç”¨åˆ°å›¾åƒä¸Š
                masked_image = Image.composite(
                    image, Image.new("RGB", image.size, (0, 0, 0)), mask
                )

                # å¦‚æœéœ€è¦è¿”å› numpy æ•°ç»„æ ¼å¼çš„ç»“æœ
                return np.array(masked_image)

            def handle_network_imageprocess(image_array):
                """Placeholder function for network image processing"""
                # send http request to server with image_array
                import requests
                import json
                import base64
                from io import BytesIO

                # Convert the image array to PIL Image
                image = Image.fromarray(image_array).convert("RGB")
                # Convert the PIL Image to base64 string
                buffered = BytesIO()
                image.save(buffered, format="PNG")
                img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")

                # NOTE Send the image to the server using request.files['image'] format
                response = requests.post(
                    "http://localhost:9090/process_image",  # Replace with your server URL
                    files={"image": buffered.getvalue()},
                )

                if response.status_code == 200:
                    # Assuming the server returns a processed image in base64 format
                    response_data = response.json()
                    img_data = response_data["image_data"]
                    img_bytes = base64.b64decode(img_data)
                    image_array = np.array(Image.open(BytesIO(img_bytes)))
                    return image_array, "caption test"
                return None

            # Event binding (dynamic based on Radio)

            def handle_image_caption():
                """Handle image captioning"""
                if SELECT_IMAGE_PATH:
                    print(f"Processing image: {SELECT_IMAGE_PATH}")
                    caption = image_captioning((SELECT_IMAGE_PATH))
                    return caption
                return "No image selected"

            process_btn.click(
                fn=handle_image_caption,  # Placeholder function for processing
                inputs=None,
                outputs=[caption_txt],
            )

        # NOTE UPLOAD FUNCTIONALITY
        # å®šä¹‰ä¿å­˜ç›®å½•
        SAVE_DIR = "./examples/photos"
        os.makedirs(SAVE_DIR, exist_ok=True)  # å¦‚æœç›®å½•ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»º

        def save_uploaded_image(image):
            """ä¿å­˜ä¸Šä¼ çš„å›¾åƒåˆ°æŒ‡å®šç›®å½•"""
            # éšæœºç”Ÿæˆ64ä½å“ˆå¸Œå­—ç¬¦ä¸²
            image_name = hashlib.md5(os.urandom(16)).hexdigest()[:32] + ".png"
            file_path = os.path.join(SAVE_DIR, image_name)  # å›ºå®šä¿å­˜ä¸ºä¸€ä¸ªæ–‡ä»¶å
            image.save(file_path)  # ä½¿ç”¨ PIL çš„ save æ–¹æ³•ä¿å­˜å›¾åƒ
            return file_path  # è¿”å›ä¿å­˜çš„æ–‡ä»¶è·¯å¾„

        def update_gallery():
            """æ›´æ–°å›¾åƒåº“"""
            # è·å–ä¿å­˜ç›®å½•ä¸­çš„æ‰€æœ‰å›¾åƒæ–‡ä»¶è·¯å¾„
            image_files = [
                os.path.join(SAVE_DIR, f)
                for f in os.listdir(SAVE_DIR)
                if f.endswith((".png", ".jpg", ".jpeg"))
            ]
            return image_files  # è¿”å›å›¾åƒæ–‡ä»¶åˆ—è¡¨

        # å¢åŠ ä¸Šä¼ ç»„ä»¶

        def handle_upload(image):
            """å¤„ç†ä¸Šä¼ çš„å›¾åƒå¹¶ä¿å­˜ï¼ŒåŒæ—¶æ›´æ–°å›¾åƒåº“"""
            saved_path = save_uploaded_image(image)  # ä¿å­˜ä¸Šä¼ çš„å›¾åƒ
            updated_gallery = update_gallery()  # æ›´æ–°å›¾åƒåº“
            return (
                f"å›¾åƒå·²ä¿å­˜åˆ°: {saved_path}",
                updated_gallery,
            )  # è¿”å›æ–‡æœ¬å’Œæ›´æ–°åçš„å›¾åƒåˆ—è¡¨

        upload.upload(
            fn=handle_upload,
            inputs=upload,
            outputs=[upload_txt, gallery],  # æ–‡æœ¬è¾“å‡º  # å›¾åƒåº“è¾“å‡º
        )

    gr.Markdown("# ğŸ“· è‡ªåŠ¨æŸ¥é‡ demo")
    gr.Markdown("å¯¹åº“å†…æ‰€æœ‰å›¾åƒè¿›è¡Œ")
    with gr.Row():

        dedup_btn = gr.Button("å¼€å§‹æŸ¥é‡")

    with gr.Row():
        dup_result = gr.Gallery(
            label="æŸ¥é‡ç»“æœ",
            columns=20,
            height=200,
            object_fit="cover",
            interactive=False,
        )

    def dedup_processing():
        """Deduplication processing function"""
        # Placeholder for deduplication logic
        # Here we just return the same images for demonstration purposes
        image_path_dir = SAVE_DIR
        from imagededup.methods import CNN
        from imagededup.methods import PHash

        cnn_encoder = CNN()

        # process
        def parse_dup_json(json_data):

            # json_data = json.loads(open('my_duplicates.json').read())
            dup_class_list = []
            for key in json_data.keys():
                in_flag = False
                for item in dup_class_list:
                    if key in item:
                        in_flag = True
                        break
                if not in_flag and len(json_data[key]) > 0:
                    class_list = []
                    class_list.append(key)
                    for sub_item in json_data[key]:
                        class_list.append(sub_item)
                    dup_class_list.append(class_list)

            print("dup lists", dup_class_list)
            res_list = []
            for item in dup_class_list:
                for sub_item in item:
                    res_list.append(sub_item)

            print("res_list:", res_list)
            return res_list

        res_vec = []
        res_vec = PHash().find_duplicates(
            image_dir=image_path_dir,
            # min_similarity_threshold=0.85,
            # outfile="output/my_duplicates_to_remove.json",
        )
        res_vec = parse_dup_json(res_vec)
        # image_paths = [os.path.join(image_path_dir, item) for item in res_vec]
        res_LIST = [os.path.join(image_path_dir, item) for item in res_vec]
        print(f"Deduplication results: {res_LIST}")
        # SHUFFULE
        # import random

        # random.shuffle(res_LIST)
        # print(f"Deduplication results: {res_LIST}")
        return res_LIST

    dedup_btn.click(
        fn=dedup_processing,  # Placeholder function for deduplication
        inputs=None,
        outputs=[dup_result],  # Update the gallery with deduplicated images
    )

    # NOTE  TAgging Grouping
    gr.Markdown("# ğŸ“· èšç±» åˆ†ç±» demo")
    gr.Markdown("å›¾åƒæ£€ç´¢")
    with gr.Row():
        with gr.Column(scale=5):
            txxt_input = gr.Textbox(
                label="è¾“å…¥æ–‡æœ¬", placeholder="Tag to search", value="a photo of person"
            )
        with gr.Column(scale=2):
            txt_btn = gr.Button("æŸ¥è¯¢")
    with gr.Row():
        gallery_output = gr.Gallery(
            label="æŸ¥è¯¢ç»“æœ",
            columns=20,
            height=200,
            object_fit="cover",
            interactive=False,
        )
    from utils.tagging import tagging_and_grouping

    def handle_text_search(text):
        """Handle text search and return matching images"""
        # Placeholder for text search logic
        # Here we just return the same images for demonstration purposes
        image_path_dir = SAVE_DIR
        # image_paths = glob.glob(os.path.join(image_path_dir, "*.png")) + glob.glob(
        #     os.path.join(image_path_dir, "*.jpg")
        # )

        res_vec = tagging_and_grouping(
            image_path_dir,
            text,  # Use the input text as the label prompt
        )
        image_paths = res_vec.get(text)
        print(f"Search results for '{text}': {image_paths}")

        # For demo, return all images
        return image_paths

    txt_btn.click(
        fn=handle_text_search,  # Placeholder function for text search
        inputs=txxt_input,
        outputs=[gallery_output],  # Update the gallery with search results
    )

    gr.Markdown("äººè„¸è‡ªåŠ¨èšç±»")
    with gr.Row():
        # with gr.Column(scale=2):
        # figure_image = gr.Image(label="å¾…èšç±»å›¾åƒ", type="pil")
        cluster_btn = gr.Button("è·å–äººè„¸èšç±»")
    # with gr.Column(scale=5):
    with gr.Row():
        cluster_gallery = gr.Gallery(
            label="äººè„¸èšç±»ç»“æœ",
            columns=20,
            height=200,
            object_fit="cover",
            interactive=False,
        )

        def handle_face_clustering():
            """Handle face clustering and return clustered images"""
            # Placeholder for face clustering logic
            # Here we just return the same images for demonstration purposes
            image_path_dir = SAVE_DIR
            from utils.face_detection_grouping import cluster_faces
            import shutil

            output_dir = "./examples/clustered_faces"
            tmp_dir = "./examples/tmp_faces"

            # remove if exists
            if os.path.exists(output_dir):
                shutil.rmtree(output_dir)
            if os.path.exists(tmp_dir):
                shutil.rmtree(tmp_dir)

            os.makedirs(output_dir, exist_ok=True)
            os.makedirs(tmp_dir, exist_ok=True)

            # Perform clustering
            cluster_faces(
                image_dir=image_path_dir,
                output_dir=output_dir,
                face_crop_dir=tmp_dir,  # Optional, set to None to skip saving cropped faces
                eps=0.2,
                min_samples=2,
            )

            FACE_CLUSTERING_ROOT = "./examples/clustered_faces"
            res_list = []
            for item in glob.glob(os.path.join(FACE_CLUSTERING_ROOT, "*", "*")):
                if item.endswith(".jpg") or item.endswith(".png"):
                    res_list.append(item)

            # Load clustered images
            # clustered_images = glob.glob(os.path.join(output_dir, "*.jpg"))
            return res_list

        cluster_btn.click(
            fn=handle_face_clustering,  # Placeholder function for clustering
            inputs=None,
            outputs=[cluster_gallery],  # Update the gallery with clustering results
        )

    gr.Markdown("# ğŸ‘— Vitural Try On")
    with gr.Row():
        tryon_btn = gr.Button("å¼€å§‹è¯•ç©¿")
    with gr.Row():
        image_human = gr.Image(
            label="è¯•ç©¿äººåƒ", type="pil", interactive=True, height=300
        )
        image_clothes = gr.Image(
            label="è¯•ç©¿è¡£æœ", type="pil", interactive=True, height=300
        )
        image_result = gr.Image(
            label="è¯•ç©¿ç»“æœ", type="pil", interactive=False, height=300
        )
    import time

    def handle_tryon(human, clothes):
        """Handle virtual try-on processing"""
        time.sleep(12.5 + 5 * random.random())  # Simulate processing time

        return "/home/zhihao/cs272project/Gradio/examples/tryon_result/00006_00_06396_00.jpg"

    tryon_btn.click(
        fn=handle_tryon,
        inputs=[image_human, image_clothes],
        outputs=[image_result],
    )
    gallery.select(handle_selection, None, [selected_image, image_human])
    gr.Markdown("# ğŸ­ Changing Face")
    with gr.Row():
        tryon_btn = gr.Button("å¼€å§‹æ¢è„¸")
    with gr.Row():
        image_human = gr.Image(label="æºäººç‰©", type="pil", interactive=True, height=300)
        image_clothes = gr.Image(
            label="é©±åŠ¨äººç‰©", type="pil", interactive=True, height=300
        )
        image_result = gr.Image(
            label="æ¢è„¸ç»“æœ", type="pil", interactive=False, height=300
        )
    import time

    def handle_face(human, clothes):
        """Handle virtual try-on processing"""
        time.sleep(7.5 + 5 * random.random())  # Simulate processing time

        return "/home/zhihao/cs272project/Gradio/examples/tryon_result/result.png"

    tryon_btn.click(
        fn=handle_face,
        inputs=[image_human, image_clothes],
        outputs=[image_result],
    )
    gallery.select(handle_selection, None, [selected_image, image_human])
    gr.Markdown("# ğŸ¤– VLLM")
    import random
    from utils.Qwen_chat import chat_with_images
    import json

    with gr.Row():

        def random_resp(message, history):
            print("=======")
            print(SELECT_IMAGE_PATH)

            messages.append(
                {"role": "user", "content": [{"type": "text", "text": message}]}
            )
            reply = chat_with_images([SELECT_IMAGE_PATH], messages)
            messages.append(
                {"role": "assistant", "content": [{"type": "text", "text": reply}]}
            )
            print("=======")
            # print(json.dumps(messages, ensure_ascii=False, indent=2))
            return reply

        gr.ChatInterface(random_resp)


if __name__ == "__main__":
    messages = []
    app.launch(server_port=8081, share=False)
